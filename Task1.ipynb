{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.16.2+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.__version__\n",
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ETL: Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 2000/2000 [00:00<00:00, 25017.10 examples/s]\n",
      "Generating train split: 100%|██████████| 979109/979109 [00:00<00:00, 2446920.17 examples/s]\n",
      "Generating validation split: 100%|██████████| 2000/2000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 979109\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"opus100\", \"en-si\")  # English-Sinhala translation\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetInfo(description='', citation='', homepage='', license='', features={'translation': Translation(languages=['en', 'si'], id=None)}, post_processed=None, supervised_keys=None, builder_name='parquet', dataset_name='opus100', config_name='en-si', version=0.0.0, splits={'test': SplitInfo(name='test', num_bytes=271735, num_examples=2000, shard_lengths=None, dataset_name='opus100'), 'train': SplitInfo(name='train', num_bytes=114950891, num_examples=979109, shard_lengths=None, dataset_name='opus100'), 'validation': SplitInfo(name='validation', num_bytes=271236, num_examples=2000, shard_lengths=None, dataset_name='opus100')}, download_checksums={'hf://datasets/opus100@805090dc28bf78897da9641cdf08b61287580df9/en-si/test-00000-of-00001.parquet': {'num_bytes': 154795, 'checksum': None}, 'hf://datasets/opus100@805090dc28bf78897da9641cdf08b61287580df9/en-si/train-00000-of-00001.parquet': {'num_bytes': 65815918, 'checksum': None}, 'hf://datasets/opus100@805090dc28bf78897da9641cdf08b61287580df9/en-si/validation-00000-of-00001.parquet': {'num_bytes': 153447, 'checksum': None}}, download_size=66124160, post_processing_size=None, dataset_size=115493862, size_in_bytes=181618022)\n"
     ]
    }
   ],
   "source": [
    "# credits for the dataset \n",
    "# Dataset Source: OPUS-100 (https://huggingface.co/datasets/opus100)\n",
    "# Configuration: English-Sinhala (en-si)\n",
    "from datasets import load_dataset_builder\n",
    "\n",
    "builder = load_dataset_builder(\"opus100\", \"en-si\")\n",
    "print(builder.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'Okay.', 'si': 'හරි, ඔයාලා වැඩකරගෙන යන මොකක් වුණත්...'}\n"
     ]
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "print(dataset['train'][333]['translation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "979109\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# confirm again the size of the dataset\n",
    "test_size = dataset[\"test\"].num_rows\n",
    "print(test_size)\n",
    "train_size = dataset[\"train\"].num_rows\n",
    "print(train_size)\n",
    "validation_size = dataset[\"validation\"].num_rows\n",
    "print(validation_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import sentence_tokenize, indic_tokenize\n",
    "from datasets import load_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"opus100\", \"en-si\")\n",
    "\n",
    "# Define source (English) and target (Sinhala) languages\n",
    "SRC_LANGUAGE = \"en\"\n",
    "TRG_LANGUAGE = \"si\"\n",
    "\n",
    "# Load tokenizer for English (spaCy) and Sinhala (Indic NLP)\n",
    "token_transform = {}\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "def sinhala_tokenizer(text):\n",
    "    return indic_tokenize.trivial_tokenize(text, lang='si')\n",
    "\n",
    "token_transform[TRG_LANGUAGE] = sinhala_tokenizer\n",
    "\n",
    "# Function to yield tokenized sentences from training data\n",
    "def yield_tokens(data, language):\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[\"translation\"][language])\n",
    "\n",
    "# Define special tokens\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text to integers (Numericalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence:  -Okay.\n",
      "Tokenized:  ['-Okay', '.']\n",
      "Numericalized:  [5199, 4]\n",
      "Sinhala Sentence:  -හරි.ඔයා දැන් යන්න ඕනේ...\n",
      "Tokenized:  ['-', 'හරි', '.', 'ඔයා', 'දැන්', 'යන්න', 'ඕනේ', '.', '.', '.']\n",
      "Numericalized:  [7, 23, 4, 9, 30, 26, 47, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary from training set\n",
    "vocab_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(\n",
    "        yield_tokens(dataset[\"train\"], ln),\n",
    "        min_freq=2,\n",
    "        specials=special_symbols,\n",
    "        special_first=True\n",
    "    )\n",
    "\n",
    "# Set default index for unknown words\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)\n",
    "\n",
    "# Example: Tokenizing and numericalizing a sentence\n",
    "sample_train = dataset[\"train\"][300][\"translation\"]\n",
    "print(\"English Sentence: \", sample_train[SRC_LANGUAGE])\n",
    "print(\"Tokenized: \", token_transform[SRC_LANGUAGE](sample_train[SRC_LANGUAGE]))\n",
    "print(\"Numericalized: \", vocab_transform[SRC_LANGUAGE](token_transform[SRC_LANGUAGE](sample_train[SRC_LANGUAGE])))\n",
    "\n",
    "print(\"Sinhala Sentence: \", sample_train[TRG_LANGUAGE])\n",
    "print(\"Tokenized: \", token_transform[TRG_LANGUAGE](sample_train[TRG_LANGUAGE]))\n",
    "print(\"Numericalized: \", vocab_transform[TRG_LANGUAGE](token_transform[TRG_LANGUAGE](sample_train[TRG_LANGUAGE])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45, 18, 13, 0, 13]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
