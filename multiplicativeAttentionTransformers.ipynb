{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.16.2+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "from indicnlp.tokenize import sentence_tokenize, indic_tokenize\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.__version__\n",
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading\n",
    "# Load the full dataset\n",
    "dataset = load_dataset(\"opus100\", \"en-si\")\n",
    "print(\"Original size of dataset: \", dataset)\n",
    "\n",
    "# Reduce the training set to 10,000 samples\n",
    "# Otherwise I can't finish and submit this assientment!!!\n",
    "# I try to maintain the ratio of 0.2 (traning set and test set)\n",
    "small_train_dataset = dataset[\"train\"].select(range(10000))\n",
    "small_test_dataset = dataset[\"test\"].select(range(2000))\n",
    "small_validation_dataset = dataset[\"validation\"].select(range(2000))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": small_train_dataset,\n",
    "    \"test\": small_test_dataset,\n",
    "    \"validation\": small_validation_dataset\n",
    "})\n",
    "\n",
    "# Verify the new sizes\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocessing: Tokenization and Numericalization\n",
    "SRC_LANGUAGE = \"en\"\n",
    "TRG_LANGUAGE = \"si\"\n",
    "\n",
    "token_transform = {}\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "def sinhala_tokenizer(text):\n",
    "    return indic_tokenize.trivial_tokenize(text, lang='si')\n",
    "\n",
    "token_transform[TRG_LANGUAGE] = sinhala_tokenizer\n",
    "\n",
    "# Function to yield tokenized sentences from training data\n",
    "def yield_tokens(data, language):\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[\"translation\"][language])\n",
    "\n",
    "# Define special tokens\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n",
    "# Build vocabulary from training set\n",
    "vocab_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(\n",
    "        yield_tokens(dataset[\"train\"], ln),\n",
    "        min_freq=2,\n",
    "        specials=special_symbols,\n",
    "        special_first=True\n",
    "    )\n",
    "\n",
    "# Set default index for unknown words\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)\n",
    "\n",
    "sample_si = dataset[\"train\"][10][\"translation\"][\"si\"]\n",
    "print(\"Raw Sinhala:\", sample_si)\n",
    "print(\"Tokenized:\", sinhala_tokenizer(sample_si))\n",
    "\n",
    "# Save vocabulary to files (ADDED THIS SECTION)\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    torch.save(vocab_transform[ln], f'vocab_{ln}.pt')\n",
    "    print(f\"Saved {ln} vocabulary to 'vocab_{ln}.pt'\")\n",
    "\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    \n",
    "    for item in batch:\n",
    "        src_sample = item[\"translation\"][SRC_LANGUAGE]  # Extract English sentence\n",
    "        trg_sample = item[\"translation\"][TRG_LANGUAGE]  # Extract Sinhala sentence\n",
    "\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch\n",
    "\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 32\n",
    "train_loader = torch.utils.data.DataLoader(dataset[\"train\"], batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset[\"validation\"], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = torch.utils.data.DataLoader(dataset[\"test\"], batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "train_loader_length = len(train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
